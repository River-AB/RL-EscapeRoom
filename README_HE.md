# תיעוד פרויקט

## [cite_start]חשוב: ספריות נדרשות להרצה [cite: 1]
[cite_start]בנוסף, הקלטות של חדרים 1-3 [cite: 1]

## [cite_start]התקנה והרצה של המשחק: [cite: 2]

1.  [cite_start]**חילוץ התיקייה של הפרויקט למחשב** [cite: 3]
2.  [cite_start]**ניווט אל התקייה דרך PowerShell** [cite: 4]
3.  [cite_start]**פתיחת סביבה וירטואלית על מנת להכיל את הסביבה המדויקת** [cite: 5]
    ```bash
    py -3.11 -m venv venv
    ```
4.  [cite_start]**הרצת הפקודה שמפעילה את הסביבה הוירטואלית** [cite: 6]
    ```bash
    ./venv/Scripts/Activate.ps1
    ```
5.  [cite_start]**ייתכן שלפניכן תצטרכו להריץ את הפקודה** [cite: 7]
    ```bash
    Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope Process
    ```
6.  [cite_start]**התקנת הספריות הנדרשות** [cite: 9]
    ```bash
    pip install -r requirements.txt
    ```
7.  [cite_start]**הרצת הקובץ הראשי של הפרויקט** [cite: 10]
    ```bash
    python main.py
    ```
    [cite_start]פקודה זאת תפעיל את חדרים. [cite: 10]

## [cite_start]הוראות שימוש ומסכים: [cite: 11]

* [cite_start]כאשר המשחק נטען מופיעה הרשת עם החדר הראשון, ובסמוך לרשת ישנו console עם הודעות על הפעולות הנלקחות במשחק. [cite: 12]
* [cite_start]בכל חדר יש אפשרות לרענן את החדר ולקבל layout חדש שמאותחל באופן רנדומי (ייתכנו איטרציות של חדרים לא פתירים). [cite: 13]
* [cite_start]לכל חדר יש אפשרות לערוך את ההגדרות של אותו חלל ואת אופן הלמידה של הסוכן. [cite: 14]
* [cite_start]לכל סוכן ישנה אפשרות לאמן באופן "איטי" או "מהיר", כאשר באופן איטי ניתן לראות את האיטרציות בחדר 1, ובחדרים 2,3 ניתן לראות את האפיסודות מתרחשות. [cite: 15]
* [cite_start]בחדרים 2-3, אם בוחרים באימון האיטי, ישנה אפשרות גם לדלג לאפיסודה כלשהי בטווח האימון (נניח מאמנים 5000 אפיסודות, אז אפשר לדלג לאפיסודות 2-4999). [cite: 16]
* [cite_start]ניתן להריץ את אותה אפיסודה על מנת לראות את מצב הלימוד של הסוכן באותה נקודה. [cite: 17]
* [cite_start]כמו כן, בכל החדרים ניתן להריץ את האימון הסופי על ידי לחיצה על כפתור `Run`. [cite: 18]
* [cite_start]ניתן לעצור את האנימציה בכפתור `Pause`, וניתן לאפס את הריצה בכפתור `Reset Run`. [cite: 19]
* [cite_start]בחדרים 2,3 ניתן ללחוץ על כפתור ה-`Show Q-Vals`, כפתור זה הוא toggle, הוא יראה על גבי הרשת את ה-q-table שאומן לאותו מצב (State). [cite: 20]
* [cite_start]בחדרים 2,3 ניתן ללחוץ על כפתור `Plot Data`, זה יפתח חלון (רצוי להגדיל אותו למסך מלא כדי לראות בנוחות את הכפתורים במסך) שבו ניתן לראות 3 גרפים שונים: [cite: 21]
    * [cite_start]הראשון מתייחס לפרסים לאורך האימון. [cite: 21]
    * [cite_start]השני למספר הצעדים לאורך האימון. [cite: 21]
    * [cite_start]השלישי מראה לנו את הרשת וכמה צעדים הסוכן לקח לכל כיוון בכל אריח. [cite: 21]

## [cite_start]חדר 1: תכנות דינאמי (Dynamic Programming) [cite: 22]

[cite_start]הסוכן הראשון הוא מסוג תכנות דינאמי, בעל מודל של הסביבה והחוקים שלה. [cite: 23] [cite_start]הוא לא נדרש לחקור את הסביבה אלא לחשב באמצעות המודל הנתון את המדיניות האידיאלית שתאפשר לו לעמוד במשימה בהתאם לחוקים. [cite: 24] [cite_start]הוא מתחיל עם מדיניות רנדומלית כלשהי ומשם מתחיל את החישובים על מנת לבצע Policy Iteration. [cite: 25]

### [cite_start]האתגר: [cite: 26]

[cite_start]הסוכן חייב לנווט ברשת כדי לאסוף תחילה את התיק, אחר כך חבל, ולבסוף להגיע ליציאה. [cite: 27] [cite_start]האתגר המרכזי הוא קיום אריחים בתצורת קירות ואריחים חלקלקים. [cite: 28] [cite_start]באריחים החלקלקים הפעולה שנבחרה על ידי הסוכן אינה משמעותית, והתוצאה מתקבלת לפי הסתברות קבועה (מוגרל רנדומלי בכל ג'ינרוט חדש של המפה, כאשר סף המינימום לכיוון חוקי, כלומר לא לכיוון קיר או לכיוון שחיצוני לרשת הוא 20%, על מנת למנוע סיכויים נמוכים מאד בכיוונים מסוימים). [cite: 28] [cite_start]הסוכן נאלץ לפתח מדיניות שלוקחת בחשבון את חוסר הוודאות הזאת. [cite: 29]

[cite_start]האריחים החלקלקים היוו בעיה במימוש באופן שבו הכיוון בהם הופיע כאשר הסוכן מחשב את המדיניות, כיוון שלהחלטה שלו אין שום משמעות, ולכן אין משמעות אמיתית לכיוון שבו הוא "היה הכי רוצה" להתקדם בו. [cite: 30] [cite_start]לכן הוספתי מנגנון לאריחים אלו, שלמרות שאין חשיבות לבחירה, המדיניות המוצגת תהיה כך שכאשר יש התכנסות של המדיניות החיצים המופיעים על האריחים (כלומר, לאן הסוכן אמור לרצות ללכת) יפנה לאריח החוקי הקרוב ביותר, שלו הערך החזוי של הפרס הוא הגבוה ביותר. [cite: 30]

[cite_start]כמו כן, הגבלתי את המיקום הרנדומלי של הפריטים יחסית למרכז המפה. [cite: 31]

### [cite_start]שיטת הלמידה: Policy Iteration [cite: 32]

[cite_start]האלגוריתם חוזר על שני שלבים עד שהמדיניות מתייצבת: [cite: 33]

1.  [cite_start]**הערכת מדיניות (Policy Evaluation)** – חישוב פונקציית הערך $V(s)$ עבור המדיניות הנוכחית, ברגע שהדלתא בסף הנדרש הלולאה יכולה לעצור (המימוש מתבצע באמצעות לולאת `while`). [cite: 34]
2.  [cite_start]**שיפור מדיניות (Policy Improvement)** – עבור כל מצב, בוחרים פעולה המובילה לתגמול עתידי מירבי, בהתאם לפונקציית הערך שחושבה קודם. [cite: 35]

[cite_start]במידה ומתבצע לפחות שינוי אחד במדיניות באותה איטרציה, האלגוריתם ימשיך לבחון שיפורים, עד שיקבל 2 מדיניות זהות ברצף. [cite: 36]

### [cite_start]מרחב המצבים: [cite: 37]

[cite_start]המימוש של המצבים הוא בצורה הבאה: [cite: 38]
[cite_start]`state:(row, col, has_bag, has_rope)` [cite: 39]
[cite_start]מימוש זה יוצר למעשה מדיניות בעלת ארבעה רבדים שונים: [cite: 39]

* [cite_start]פעולה כשלסוכן אין אף פריט. [cite: 40]
* [cite_start]פעולה כשיש רק תיק. [cite: 41]
* [cite_start]פעולה כשיש רק חבל. [cite: 42]
* [cite_start]פעולה כשיש את שניהם. [cite: 43]

**ההבנה של הסדר:** הסוכן לא תוכנת לאסוף את התיק קודם. [cite_start]הוא למד זאת בעצמו. [cite: 44] [cite_start]מערכת התגמול בנויה כך שאם יאסוף את החבל לפני התיק, או יגיע לנקודת הסיום ללא 2 הפריטים הוא ייענש. [cite: 45] [cite_start]המערכת נתנה לו פרס על איסוף החבל רק אם כבר היה לו את התיק. [cite: 45] [cite_start]לכן, בחישובים שלו, הוא גילה שהמסלול שבו יאסוף את התיק, לאחר מכן את החבל ורק אז יגיע לנקודת הסיום נותן ניקוד סופי גבוה משמעותית מכל מסלול אחר. [cite: 46]

### [cite_start]תגמולים: [cite: 47]

* [cite_start]על איסוף התיק הסוכן מקבל 20 נקודות. [cite: 48]
* [cite_start]על איסוף החבל ללא התיק הסוכן מקבל עונש של -10 נקודות, אך אם כבר מחזיק בתיק מקבל פרס של 30 נקודות. [cite: 49]
* [cite_start]על הגעה לנקודת הסיום ללא שני הפריטים הסוכן נענש ב-20 נקודות, ואם מגיע לסיום עם שניהם מקבל פרס של 100 נקודות. [cite: 50]

### [cite_start]פרמטרים ניתנים לשליטה: [cite: 51]

* [cite_start]**פקטור הנחה (גאמה)** – חשיבות התגמולים העתידיים (דיפולטיבי 0.9). [cite: 52]
* [cite_start]**Theta** – סף התכנסות של שלב הערכת המדיניות (הסף שהדלתא צריך לרדת ממנו) (דיפולטיבי 0.000001). [cite: 53]
* [cite_start]**פריטים התחלתיים** – ניתן לסמן האם הסוכן מתחיל עם התיק, החבל או שניהם. [cite: 54]
* [cite_start]**כמות הקירות או האריחים החלקלקים** [cite: 55]

## [cite_start]חדר 2: SARSA [cite: 56]

[cite_start]מימוש של אלגוריתם SARSA, כך שהסוכן מודע לכך שהוא לא תמיד יפעל באופן האופטימלי והוא חוקר מטבעו, כך שייתכן שהצעד הבא שלו הוא רנדומי. [cite: 57] [cite_start]מימוש שכזה יוצר מצב של סוכן שיותר "מודע" למגבלות שלו ושהוא עלול "למעוד", ולכן הוא לומד גישות שהן יותר בטוחות וקונסרבטיביות, במיוחד ליד סכנות הסביבה. [cite: 58]

### [cite_start]האתגר: [cite: 59]

[cite_start]בחדר זה, הרשת מחולקת ל-3 אזורים מופרדים. [cite: 60] [cite_start]באזור הראשון יש אויב שמסייר במסלול מלבני קבוע מסביב לאזור הראשון. [cite: 60] [cite_start]הסוכן צריך ללמוד לאסוף את המפתח שנמצא באזור זה, אשר פותח קיר רנדומי כלשהו אשר מפריד בין החלל הראשון לשני. [cite: 61] [cite_start]הוא צריך ללמוד לעבור לחלל השני ולהיכנס שם דרך מנהרה שתוביל אותו לחלל השלישי והאחרון, בו נמצאת נקודת היציאה. [cite: 62]

[cite_start]בחדר זה מפוזרים באופן רנדומי קירות ואריחים חלקלקים (בחדר זה הסיכוי להחליק לכל כיוון חוקי שווה לכל כיוון, וכך גם בחדר הבא), אך המבנה הכללי של שלושת החללים הינו קבוע. [cite: 63] [cite_start]הדרך שבה מסייר האויב הינה קבועה ותמיד תכלול אריחים רגילים בלבד (ואת משבצת ההתחלה). [cite: 64]

[cite_start]ייתכן שתופק מפה בלתי פתירה מבחינת הסוכן, לדוגמה שהמעבר בין החלל הראשון לשני חסום עם קיר רנדומי, או שהדרך למנהרה חסומה. [cite: 65] [cite_start]כנ"ל לגבי החלל השלישי, ייתכן שהדרך מהיציאה של המנהרה אל משבצת הסיום חסומה גם כן. [cite: 66]

### [cite_start]שיטת הלמידה: [cite: 67]

[cite_start]אלגוריתם SARSA הוא ללא מודל (Model-Free) ומבוסס-מדיניות (On-Policy): [cite: 68]

* [cite_start]**ללא מודל** – הכוונה לומד על ידי ניסיון בפועל, ללא צורך לדעת את הסתברויות המעבר. [cite: 69]
* [cite_start]**מבוסס מדיניות** – הכוונה ללומד את הערכים של המדיניות הנוכחית, כולל פעולות אקראיות. [cite: 70] [cite_start]האלגוריתם מעדכן את ערך Q לפי $(s, a, r, s', a')$ כאשר $a'$ היא הפעולה שנבחרה בפועל במצב הבא. [cite: 70]

### [cite_start]מרחב המצבים: [cite: 71]

[cite_start]המימוש של המצבים הוא בצורה הבאה: [cite: 72]
[cite_start]`state:(row, col, has_key)` [cite: 73]
[cite_start]מימוש זה יוצר למעשה מדיניות בעלת שני רבדים שונים: [cite: 73]

* [cite_start]פעולה כשלסוכן אין את המפתח. [cite: 74]
* [cite_start]פעולה כאשר אסף את המפתח. [cite: 75]

[cite_start]הלמידה מתבצעת באופן כזה שהסוכן מתחיל כל אפיסודה מאותה נקודה התחלתית, מתוך הבנה שאם יתחיל באופן רנדומי בחלל השני או השלישי הוא נמצא במצב לא הגיוני, שכן הוא מתחיל ללא מפתח, וכך נוצר מצב שהוא עלול להתחיל באחד מחללים אלה במצב $(r,c,0)$ שזה מצב לא הגיוני ולמידה שלו היא מיותרת. [cite: 76]

### [cite_start]תגמולים: [cite: 77]

* [cite_start]על איסוף המפתח הסוכן מקבל פרס של 50 נקודות. [cite: 78]
* [cite_start]על שימוש במנהרה הוא מקבל פרס של 5 נקודות. [cite: 79]
* [cite_start]על הגעה לנקודת הסיום הוא מקבל פרס של 100 נקודות. [cite: 80]
* [cite_start]על התנגשות בקיר או ניסיון "לצאת" מהרשת הסוכן נענש ב-5 נקודות. [cite: 81]
* [cite_start]אם הסוכן נתפס על ידי האויב הוא מקבל עונש של -100 נקודות והאפיסודה נגמרת. [cite: 82]

### [cite_start]פרמטרים ניתנים לשליטה: [cite: 83]

* [cite_start]**אלפא** – קצב הלמידה (דיפולטיבי 0.1). [cite: 84]
* [cite_start]**אפסילון** – קצב החקירה (דיפולטיבי 1). [cite: 85]
* [cite_start]**מספר האפיסודות** (דיפולטיבי 5000). [cite: 86]
* [cite_start]**מספר הצעדים המקסימלי לכל אפיסודה** (דיפולטיבי 200). [cite: 87]
* [cite_start]**אפסילון מינימלי** (0.01). [cite: 88]
* [cite_start]**מקדם דעיכה** (0.9995). [cite: 89]
* [cite_start]**מספר הקירות והאריחים החלקלקים** [cite: 90]

## [cite_start]חדר 3: Q-Learning [cite: 91]

[cite_start]הסוכן השלישי הוא "החוקר החמדן". [cite: 92] [cite_start]כמו הסוכן בחדר 2, הוא לומד מניסוי וטעייה, אבל הוא תמיד אופטימי ומניח שהצעד הבא שלו יהיה אופטימלי. [cite: 92] [cite_start]כלומר הוא אינו מניח שהוא עלול "למעוד" ולכן תמיד מחשב את התגמול לפי הפעולה האופטימלית. [cite: 93]

### [cite_start]האתגר: פתרון חידת פיזיקה מורכבת והתמודדות עם "פחד" [cite: 94]

[cite_start]בחדר זה, ישנם שני "איים" במרכז הרשת. [cite: 95] [cite_start]אי היא משבצת שמוקפת בבורות, ונפילה לבור מסיימת את האפיסודה, ומעניקה לסוכן עונש. [cite: 95] [cite_start]במרכז האי ישנה משבצת חוקית, ועליה מפתח. [cite: 96] [cite_start]על הסוכן לאסוף 2 מפתחות (סדר לא חשוב) על מנת לפתוח את הדלת שחוסמת את היציאה. [cite: 96]

[cite_start]ברחבי הרשת ישנם 2 קרשים. [cite: 97] [cite_start]הסוכן יכול למשוך או לדחוף את הקרשים, ועליו להבין שהוא יכול לדחוף את הקרשים לבור, ובעזרת החבל (לא באמת, רק לצורך הסיפור של המשחק) והקרש ליצור גשר שיאפשר לו לאסוף את המפתח. [cite: 97] [cite_start]מדובר באתגר משמעותי, כי הפרס על בניית הגשר מגיע רק לאחר ביצוע סדרת פעולות ארוכה, שלכל אחת מהן אין תגמול מיידי. [cite: 98] [cite_start]ולכן הפריסה הדיפולטיבית של המפה הינה ללא כל מכשול נוסף. [cite: 99]

[cite_start]ניתן לערוך את המפה על מנת להקשות על האתגר של הסוכן ובכך להוסיף קירות. [cite: 100] [cite_start]זה יכול לגרום להפקה של מפות שאינן פתירות או שמאד קשה לפתור אותן, ולכן במצב כזה מומלץ להעלות את כמות האפיסודות למספר משמעותי (כ-40 אלף, אולי אף יותר). [cite: 100]

[cite_start]בחדר זה היו הרבה אתגרים מבחינת אימון המודל, כאשר העיקרית בהם הייתה בעיית האופטימום המקומי, בעיה זו נוצרה בעקבות ה"פחד" של הסוכן. [cite: 101] [cite_start]הוא למד שדחיפת הקרש נותנת פרס קטן ובטוח (+5), בעוד שהתקרבות לבור מסוכנת מאוד (-100). [cite: 102] [cite_start]לכן, הוא העדיף להיתקע בפתרון ה"בסדר" והבטוח, ולא להסתכן כדי למצוא את הפתרון המושלם. [cite: 103] [cite_start]ובכך החל לדחוף קרש בלולאות עד שנגמרו הצעדים. [cite: 104] [cite_start]הפתרון היה להסיר את הפרס על דחיפת הקרש, והוספת עונש קטן נוסף על כל פעולה. [cite: 105]

### [cite_start]שיטת הלמידה: Q-Learning [cite: 106]

[cite_start]אלגוריתם Q-Learning משמש כאן – גם הוא ללא מודל, אך מסוג Off-Policy. [cite: 107]

* [cite_start]**Off-Policy** – האלגוריתם לומד את המדיניות האופטימלית בלי תלות בפעולות בפועל. [cite: 108] [cite_start]הוא משתמש בערך ה-Q המרבי של המצב הבא, מתוך הנחה שהסוכן יבחר בפעולה הטובה ביותר האפשרית. [cite: 108]

### [cite_start]מרחב המצבים: [cite: 109]

[cite_start]המימוש של המצבים הוא בצורה הבאה: [cite: 110]
[cite_start]`state:(row, col, row_plank1, col_plank1, row_plank2, col_plank2, has_silver_key, has_gold_key)` [cite: 111]

[cite_start]כלומר יש מרחב מצבים די גדול, על מנת שהסוכן "יבין" שאכן נוצר גשר ולא ניתן לדחוף יותר את הקרש. [cite: 112] [cite_start]כאשר קרש נדחף לתוך בור הערך של השורה שלו נהפך ל-1 והסוכן מקבל על כך תגמול (חיובי או שלילי בהתאם למיקום הגשר). [cite: 112] [cite_start]כאשר הערך נהיה -1 הסוכן לומד שאותה משבצת שאליה דחף את הקרש הפכה להיות חוקית וכעת הוא יכול להתנייד עליה על מנת לאסוף את המפתח. [cite: 113]

### [cite_start]תגמולים: [cite: 114]

* [cite_start]על איסוף המפתח הסוכן מקבל פרס של 50 נקודות. [cite: 115]
* [cite_start]על בניית גשר במיקום שמאפשר איסוף של מפתח הוא מקבל פרס של 30 נקודות. [cite: 116]
* [cite_start]על בניית גשר במיקום שאינו מאפשר איסוף של מפתח הוא מקבל עונש של -20 נקודות. [cite: 117]
* [cite_start]על בניית גשר לאי שכבר יש לו גשר הוא מקבל עונש של -50 נקודות. [cite: 118]
* [cite_start]על הגעה לנקודת הסיום הוא מקבל פרס של 100 נקודות. [cite: 119]
* [cite_start]על התנגשות בקיר או ניסיון "לצאת" מהרשת הסוכן נענש ב-5 נקודות. [cite: 120]
* [cite_start]אם הסוכן נופל לבור הוא מקבל עונש של -100 נקודות ונגמרת האפיסודה. [cite: 121]
* [cite_start]הסוכן בנוסף נענש ב-0.1 נקודות על כל צעד שהוא לוקח. [cite: 122] [cite_start]הסיבה שתוספת זו חיונית למימוש היא מכיוון שהסוכן עלול למצוא "מסלול בטוח" אך חסר פרסים ויעדיף את מסלול זה כי במבנה זה של החדר הוא בעיקר נענש בהתחלה, שכן להגיע לפרס ראשוני הוא אתגר משמעותי. [cite: 122] [cite_start]לכן נמנע ממנו מסלול שבו הוא מבצע את אותה פעולה בלולאה על מנת לא להסתכן בלהיענש בחקירה. [cite: 123]

### [cite_start]פרמטרים ניתנים לשליטה: [cite: 124]

* [cite_start]**אלפא** – קצב הלמידה (דיפולטיבי 0.1). [cite: 125]
* [cite_start]**אפסילון** – קצב החקירה (דיפולטיבי 1). [cite: 126]
* [cite_start]**מספר האפיסודות** (דיפולטיבי 5000). [cite: 127]
* [cite_start]**מספר הצעדים המקסימלי לכל אפיסודה** (דיפולטיבי 200). [cite: 128]
* [cite_start]**אפסילון מינימלי** (0.01). [cite: 129]
* [cite_start]**מקדם דעיכה** (0.9995). [cite: 130]
* [cite_start]**מספר הקירות** [cite: 131]